{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Credit Card Fraud Detection\n",
    "\n",
    "**Dataset:** creditcard.csv\n",
    "\n",
    "**Objective:** Comprehensive exploratory analysis to understand:\n",
    "- Dataset structure and characteristics\n",
    "- Class distribution and imbalance\n",
    "- Feature distributions and correlations\n",
    "- Temporal patterns\n",
    "- Statistical insights for fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/raw/creditcard.csv')\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values check\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = 100 * missing / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "print(missing_table[missing_table['Missing Count'] > 0])\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ No missing values detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates:,}\")\n",
    "if duplicates == 0:\n",
    "    print(\"‚úÖ No duplicates detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_pct = 100 * class_counts / len(df)\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLegitimate (Class 0): {class_counts[0]:,} ({class_pct[0]:.2f}%)\")\n",
    "print(f\"Fraudulent (Class 1): {class_counts[1]:,} ({class_pct[1]:.2f}%)\")\n",
    "print(f\"\\n‚ö†Ô∏è  Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"\\nThis is a highly imbalanced dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=Legitimate, 1=Fraud)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "for i, v in enumerate(class_counts):\n",
    "    axes[0].text(i, v + 5000, f'{v:,}\\n({class_pct[i]:.2f}%)', \n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "explode = (0, 0.1)\n",
    "axes[1].pie(class_counts, labels=['Legitimate', 'Fraud'], autopct='%1.2f%%',\n",
    "           startangle=90, colors=colors, explode=explode, shadow=True)\n",
    "axes[1].set_title('Class Distribution (Proportion)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time analysis - convert seconds to hours\n",
    "df['Time_hours'] = df['Time'] / 3600\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEMPORAL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTime range: {df['Time_hours'].min():.2f} to {df['Time_hours'].max():.2f} hours\")\n",
    "print(f\"Duration: {df['Time_hours'].max():.2f} hours (~{df['Time_hours'].max()/24:.1f} days)\")\n",
    "print(f\"\\nTime statistics for Legitimate transactions:\")\n",
    "print(df[df['Class']==0]['Time_hours'].describe())\n",
    "print(f\"\\nTime statistics for Fraudulent transactions:\")\n",
    "print(df[df['Class']==1]['Time_hours'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction distribution over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Overall transaction timeline\n",
    "axes[0].hist(df['Time_hours'], bins=100, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Transaction Distribution Over Time (All Transactions)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (hours)', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Transactions', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fraud vs Legitimate over time\n",
    "axes[1].hist([df[df['Class']==0]['Time_hours'], \n",
    "              df[df['Class']==1]['Time_hours']], \n",
    "             bins=100, color=['#2ecc71', '#e74c3c'], \n",
    "             label=['Legitimate', 'Fraud'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Transaction Distribution by Class Over Time', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time (hours)', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Transactions', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud rate over time bins\n",
    "time_bins = pd.cut(df['Time_hours'], bins=48)  # 48 bins for ~1 hour each\n",
    "fraud_rate_time = df.groupby(time_bins)['Class'].agg(['sum', 'count', 'mean'])\n",
    "fraud_rate_time.columns = ['Fraud_Count', 'Total_Transactions', 'Fraud_Rate']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "x_pos = range(len(fraud_rate_time))\n",
    "ax.plot(x_pos, fraud_rate_time['Fraud_Rate'] * 100, marker='o', \n",
    "        linewidth=2, markersize=4, color='#e74c3c')\n",
    "ax.fill_between(x_pos, fraud_rate_time['Fraud_Rate'] * 100, alpha=0.3, color='#e74c3c')\n",
    "ax.set_title('Fraud Rate Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Time Bins (~1 hour each)', fontsize=12)\n",
    "ax.set_ylabel('Fraud Rate (%)', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount statistics by class\n",
    "print(\"=\"*80)\n",
    "print(\"AMOUNT STATISTICS BY CLASS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLegitimate Transactions (Class 0):\")\n",
    "print(df[df['Class']==0]['Amount'].describe())\n",
    "print(\"\\nFraudulent Transactions (Class 1):\")\n",
    "print(df[df['Class']==1]['Amount'].describe())\n",
    "\n",
    "# Statistical test\n",
    "legit_amounts = df[df['Class']==0]['Amount']\n",
    "fraud_amounts = df[df['Class']==1]['Amount']\n",
    "t_stat, p_value = stats.mannwhitneyu(legit_amounts, fraud_amounts, alternative='two-sided')\n",
    "print(f\"\\nMann-Whitney U Test:\")\n",
    "print(f\"  Test Statistic: {t_stat:.2f}\")\n",
    "print(f\"  P-value: {p_value:.2e}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"  ‚úÖ Significant difference in amount distributions (p < 0.05)\")\n",
    "else:\n",
    "    print(\"  ‚ùå No significant difference in amount distributions (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount distribution visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Histogram - All transactions\n",
    "axes[0, 0].hist(df['Amount'], bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Amount Distribution (All Transactions)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Amount ($)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale histogram\n",
    "axes[0, 1].hist(np.log1p(df['Amount']), bins=50, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Amount Distribution (Log Scale)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log(Amount + 1)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by class\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1, 0], \n",
    "           patch_artist=True, grid=True)\n",
    "axes[1, 0].set_title('Amount Distribution by Class', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Class (0=Legitimate, 1=Fraud)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Amount ($)', fontsize=11)\n",
    "plt.suptitle('')  # Remove auto title\n",
    "\n",
    "# Violin plot\n",
    "parts = axes[1, 1].violinplot([legit_amounts, fraud_amounts], \n",
    "                              positions=[0, 1], showmeans=True, showmedians=True)\n",
    "axes[1, 1].set_title('Amount Distribution (Violin Plot)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Class (0=Legitimate, 1=Fraud)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Amount ($)', fontsize=11)\n",
    "axes[1, 1].set_xticks([0, 1])\n",
    "axes[1, 1].set_xticklabels(['Legitimate', 'Fraud'])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount distribution comparison (zoomed)\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot both distributions\n",
    "ax.hist(legit_amounts, bins=100, alpha=0.6, label='Legitimate', \n",
    "        color='#2ecc71', edgecolor='black', density=True)\n",
    "ax.hist(fraud_amounts, bins=100, alpha=0.6, label='Fraud', \n",
    "        color='#e74c3c', edgecolor='black', density=True)\n",
    "\n",
    "ax.set_title('Amount Distribution Comparison (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Amount ($)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_xlim(0, 500)  # Zoom to see most of the data\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Features Analysis (V1-V28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get V columns\n",
    "v_columns = [col for col in df.columns if col.startswith('V')]\n",
    "print(f\"Number of PCA features: {len(v_columns)}\")\n",
    "\n",
    "# Statistics for V features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PCA FEATURES STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df[v_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of V features\n",
    "fig, axes = plt.subplots(7, 4, figsize=(20, 24))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(v_columns):\n",
    "    axes[idx].hist(df[col], bins=50, color='#16a085', alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'{col} Distribution', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution of All PCA Features (V1-V28)', \n",
    "             fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V features comparison by class (sample of 6 features)\n",
    "sample_v_features = v_columns[::5][:6]  # Sample every 5th feature, take first 6\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(sample_v_features):\n",
    "    axes[idx].hist([df[df['Class']==0][col], df[df['Class']==1][col]], \n",
    "                   bins=50, alpha=0.7, label=['Legitimate', 'Fraud'],\n",
    "                   color=['#2ecc71', '#e74c3c'], edgecolor='black', density=True)\n",
    "    axes[idx].set_title(f'{col} by Class', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Density', fontsize=10)\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sample PCA Features Distribution by Class', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all features\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Correlation with target (Class)\n",
    "class_correlation = correlation_matrix['Class'].sort_values(ascending=False)\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 15 FEATURES CORRELATED WITH FRAUD\")\n",
    "print(\"=\"*80)\n",
    "print(class_correlation.head(16))  # Top 15 + Class itself\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 15 FEATURES NEGATIVELY CORRELATED WITH FRAUD\")\n",
    "print(\"=\"*80)\n",
    "print(class_correlation.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (full)\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            vmin=-1, vmax=1, annot=False)\n",
    "plt.title('Correlation Matrix - All Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with Class visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Get correlations and sort\n",
    "class_corr_sorted = class_correlation.drop('Class').sort_values()\n",
    "\n",
    "# Create color map based on positive/negative correlation\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in class_corr_sorted.values]\n",
    "\n",
    "class_corr_sorted.plot(kind='barh', ax=ax, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_title('Feature Correlation with Fraud Class', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance via Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Mann-Whitney U test for each feature\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "p_values = {}\n",
    "test_stats = {}\n",
    "\n",
    "legitimate = df[df['Class'] == 0]\n",
    "fraudulent = df[df['Class'] == 1]\n",
    "\n",
    "for col in v_columns + ['Time', 'Amount']:\n",
    "    stat, p_val = mannwhitneyu(legitimate[col], fraudulent[col], alternative='two-sided')\n",
    "    p_values[col] = p_val\n",
    "    test_stats[col] = stat\n",
    "\n",
    "# Create results dataframe\n",
    "statistical_test_results = pd.DataFrame({\n",
    "    'Feature': list(p_values.keys()),\n",
    "    'P-Value': list(p_values.values()),\n",
    "    'Test_Statistic': list(test_stats.values()),\n",
    "    'Significant': ['Yes' if p < 0.05 else 'No' for p in p_values.values()]\n",
    "}).sort_values('P-Value')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE OF FEATURES (Mann-Whitney U Test)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFeatures with significant difference (p < 0.05): {sum(statistical_test_results['Significant']=='Yes')} out of {len(statistical_test_results)}\")\n",
    "print(\"\\nTop 15 most significant features:\")\n",
    "print(statistical_test_results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize p-values\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Use negative log of p-values for better visualization\n",
    "neg_log_p = -np.log10(statistical_test_results['P-Value'])\n",
    "colors_sig = ['#e74c3c' if x == 'Yes' else '#95a5a6' \n",
    "              for x in statistical_test_results['Significant']]\n",
    "\n",
    "ax.barh(statistical_test_results['Feature'], neg_log_p, \n",
    "        color=colors_sig, alpha=0.8, edgecolor='black')\n",
    "ax.axvline(x=-np.log10(0.05), color='black', linestyle='--', \n",
    "           linewidth=2, label='Significance threshold (p=0.05)')\n",
    "ax.set_xlabel('-log10(P-Value)', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "ax.set_title('Statistical Significance of Features\\n(Higher = More Discriminative)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Top Discriminative Features Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 4 most correlated features (positive and negative)\n",
    "top_positive = class_correlation.drop('Class').nlargest(2).index.tolist()\n",
    "top_negative = class_correlation.drop('Class').nsmallest(2).index.tolist()\n",
    "top_features = top_positive + top_negative\n",
    "\n",
    "print(\"Most discriminative features:\")\n",
    "print(f\"  Positive correlation: {top_positive}\")\n",
    "print(f\"  Negative correlation: {top_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed visualization of top features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    # KDE plot\n",
    "    legitimate[feature].plot(kind='kde', ax=axes[idx], label='Legitimate', \n",
    "                             color='#2ecc71', linewidth=2.5)\n",
    "    fraudulent[feature].plot(kind='kde', ax=axes[idx], label='Fraud', \n",
    "                            color='#e74c3c', linewidth=2.5)\n",
    "    axes[idx].set_title(f'{feature} Distribution by Class\\nCorrelation: {class_correlation[feature]:.4f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Top Discriminative Features - Density Comparison', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of top 2 features\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Sample data to avoid overplotting\n",
    "sample_size = min(5000, len(fraudulent))\n",
    "legit_sample = legitimate.sample(n=sample_size, random_state=42)\n",
    "fraud_sample = fraudulent.sample(n=min(sample_size, len(fraudulent)), random_state=42)\n",
    "\n",
    "ax.scatter(legit_sample[top_features[0]], legit_sample[top_features[1]], \n",
    "          alpha=0.5, s=30, label='Legitimate', color='#2ecc71', edgecolors='black', linewidth=0.5)\n",
    "ax.scatter(fraud_sample[top_features[0]], fraud_sample[top_features[1]], \n",
    "          alpha=0.7, s=50, label='Fraud', color='#e74c3c', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(top_features[0], fontsize=12)\n",
    "ax.set_ylabel(top_features[1], fontsize=12)\n",
    "ax.set_title(f'Scatter Plot: {top_features[0]} vs {top_features[1]}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of top features (sampled)\n",
    "sample_df = pd.concat([\n",
    "    legitimate.sample(n=1000, random_state=42),\n",
    "    fraudulent.sample(n=min(1000, len(fraudulent)), random_state=42)\n",
    "])\n",
    "\n",
    "pairplot_features = top_features[:3] + ['Class']  # Top 3 features + Class\n",
    "g = sns.pairplot(sample_df[pairplot_features], hue='Class', \n",
    "                 palette={0: '#2ecc71', 1: '#e74c3c'},\n",
    "                 diag_kind='kde', plot_kws={'alpha': 0.6, 's': 40, 'edgecolor': 'black'},\n",
    "                 height=3)\n",
    "g.fig.suptitle('Pairplot of Top Discriminative Features', \n",
    "               fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   ‚Ä¢ Total transactions: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {df.shape[1]} (28 PCA features + Time + Amount)\")\n",
    "print(f\"   ‚Ä¢ No missing values: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ No duplicates: ‚úÖ\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE\")\n",
    "print(f\"   ‚Ä¢ Legitimate transactions: {class_counts[0]:,} ({class_pct[0]:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Fraudulent transactions: {class_counts[1]:,} ({class_pct[1]:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"   ‚Ä¢ ‚ö†Ô∏è  Requires special handling (SMOTE, class weights, etc.)\")\n",
    "\n",
    "print(f\"\\n3. TEMPORAL PATTERNS\")\n",
    "print(f\"   ‚Ä¢ Time span: {df['Time_hours'].max():.1f} hours (~{df['Time_hours'].max()/24:.1f} days)\")\n",
    "print(f\"   ‚Ä¢ Fraud transactions show different temporal patterns\")\n",
    "print(f\"   ‚Ä¢ Fraud rate varies over time bins\")\n",
    "\n",
    "print(f\"\\n4. AMOUNT ANALYSIS\")\n",
    "print(f\"   ‚Ä¢ Mean amount (Legitimate): ${legit_amounts.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Mean amount (Fraud): ${fraud_amounts.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median amount (Legitimate): ${legit_amounts.median():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median amount (Fraud): ${fraud_amounts.median():.2f}\")\n",
    "print(f\"   ‚Ä¢ Statistical difference: {'Yes (p<0.05)' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "print(f\"\\n5. MOST IMPORTANT FEATURES (by correlation with fraud)\")\n",
    "top_5_corr = class_correlation.drop('Class').abs().nlargest(5)\n",
    "for i, (feat, corr) in enumerate(top_5_corr.items(), 1):\n",
    "    print(f\"   {i}. {feat}: {class_correlation[feat]:.4f}\")\n",
    "\n",
    "print(f\"\\n6. STATISTICALLY SIGNIFICANT FEATURES\")\n",
    "n_significant = sum(statistical_test_results['Significant'] == 'Yes')\n",
    "print(f\"   ‚Ä¢ {n_significant}/{len(statistical_test_results)} features show significant difference (p<0.05)\")\n",
    "print(f\"   ‚Ä¢ All V features are discriminative to varying degrees\")\n",
    "\n",
    "print(f\"\\n7. RECOMMENDATIONS FOR MODELING\")\n",
    "print(f\"   ‚úì Use stratified splitting due to class imbalance\")\n",
    "print(f\"   ‚úì Apply class balancing techniques (SMOTE, class weights)\")\n",
    "print(f\"   ‚úì Focus on Precision-Recall metrics (not just accuracy)\")\n",
    "print(f\"   ‚úì Consider ensemble methods (Random Forest, XGBoost)\")\n",
    "print(f\"   ‚úì Feature scaling already done via PCA, but scale Time/Amount\")\n",
    "print(f\"   ‚úì Use cross-validation with stratification\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_stats = {\n",
    "    'Dataset': 'creditcard.csv',\n",
    "    'Total_Transactions': len(df),\n",
    "    'Legitimate_Count': class_counts[0],\n",
    "    'Fraud_Count': class_counts[1],\n",
    "    'Fraud_Rate_%': class_pct[1],\n",
    "    'Imbalance_Ratio': imbalance_ratio,\n",
    "    'Time_Span_Hours': df['Time_hours'].max(),\n",
    "    'Mean_Amount_Legitimate': legit_amounts.mean(),\n",
    "    'Mean_Amount_Fraud': fraud_amounts.mean(),\n",
    "    'Median_Amount_Legitimate': legit_amounts.median(),\n",
    "    'Median_Amount_Fraud': fraud_amounts.median(),\n",
    "    'Top_Feature_1': top_5_corr.index[0],\n",
    "    'Top_Feature_1_Correlation': class_correlation[top_5_corr.index[0]],\n",
    "    'Top_Feature_2': top_5_corr.index[1],\n",
    "    'Top_Feature_2_Correlation': class_correlation[top_5_corr.index[1]],\n",
    "    'Significant_Features_Count': n_significant\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "print(\"\\nüìä SUMMARY STATISTICS\")\n",
    "print(summary_df.T)\n",
    "\n",
    "# Optionally save to CSV\n",
    "# summary_df.to_csv('../data/processed/creditcard_eda_summary.csv', index=False)\n",
    "# print(\"\\n‚úÖ Summary saved to: ../data/processed/creditcard_eda_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of EDA\n",
    "\n",
    "**Next Steps:**\n",
    "1. Feature engineering (if needed beyond PCA features)\n",
    "2. Train-test split with stratification\n",
    "3. Apply class balancing (SMOTE)\n",
    "4. Model training and evaluation\n",
    "5. Hyperparameter tuning\n",
    "6. Model interpretation with SHAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
